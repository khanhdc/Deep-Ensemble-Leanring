{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "processed-platform",
   "metadata": {
    "id": "mature-smell"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.impute import SimpleImputer\n",
    "lr = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cubic-insider",
   "metadata": {
    "id": "compound-belief"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# reading csv files\n",
    "df =  pd.read_csv('heart.dat', sep=\" \", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "complicated-labor",
   "metadata": {
    "id": "warming-money"
   },
   "outputs": [],
   "source": [
    "num_Interation = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "agreed-danger",
   "metadata": {
    "id": "available-elder"
   },
   "outputs": [],
   "source": [
    "column_name = ['age', 'sex', 'chest pain', 'resting blood pressure', 'serum cholestoral', 'fasting blood sugar', 'resting electrocardiographic', 'maximum heart rate achieved',' exercise induced angina', 'oldpeak',' the slope of the peak exercise ST segment', 'number of major vessels', 'tha', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "exact-standard",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "earned-tattoo",
    "outputId": "f289a6e3-0efe-4cc8-c364-f4a777824b9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hindu-sellers",
   "metadata": {
    "id": "fitted-skill"
   },
   "outputs": [],
   "source": [
    "df.columns = column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "clean-haiti",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "id": "boxed-graphic",
    "outputId": "8fdd53d9-e345-46de-d811-97a571239241"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>chest pain</th>\n",
       "      <th>resting blood pressure</th>\n",
       "      <th>serum cholestoral</th>\n",
       "      <th>fasting blood sugar</th>\n",
       "      <th>resting electrocardiographic</th>\n",
       "      <th>maximum heart rate achieved</th>\n",
       "      <th>exercise induced angina</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>the slope of the peak exercise ST segment</th>\n",
       "      <th>number of major vessels</th>\n",
       "      <th>tha</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>564.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>270 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  sex  chest pain  resting blood pressure  serum cholestoral  \\\n",
       "0    70.0  1.0         4.0                   130.0              322.0   \n",
       "1    67.0  0.0         3.0                   115.0              564.0   \n",
       "2    57.0  1.0         2.0                   124.0              261.0   \n",
       "3    64.0  1.0         4.0                   128.0              263.0   \n",
       "4    74.0  0.0         2.0                   120.0              269.0   \n",
       "..    ...  ...         ...                     ...                ...   \n",
       "265  52.0  1.0         3.0                   172.0              199.0   \n",
       "266  44.0  1.0         2.0                   120.0              263.0   \n",
       "267  56.0  0.0         2.0                   140.0              294.0   \n",
       "268  57.0  1.0         4.0                   140.0              192.0   \n",
       "269  67.0  1.0         4.0                   160.0              286.0   \n",
       "\n",
       "     fasting blood sugar  resting electrocardiographic  \\\n",
       "0                    0.0                           2.0   \n",
       "1                    0.0                           2.0   \n",
       "2                    0.0                           0.0   \n",
       "3                    0.0                           0.0   \n",
       "4                    0.0                           2.0   \n",
       "..                   ...                           ...   \n",
       "265                  1.0                           0.0   \n",
       "266                  0.0                           0.0   \n",
       "267                  0.0                           2.0   \n",
       "268                  0.0                           0.0   \n",
       "269                  0.0                           2.0   \n",
       "\n",
       "     maximum heart rate achieved   exercise induced angina  oldpeak  \\\n",
       "0                          109.0                       0.0      2.4   \n",
       "1                          160.0                       0.0      1.6   \n",
       "2                          141.0                       0.0      0.3   \n",
       "3                          105.0                       1.0      0.2   \n",
       "4                          121.0                       1.0      0.2   \n",
       "..                           ...                       ...      ...   \n",
       "265                        162.0                       0.0      0.5   \n",
       "266                        173.0                       0.0      0.0   \n",
       "267                        153.0                       0.0      1.3   \n",
       "268                        148.0                       0.0      0.4   \n",
       "269                        108.0                       1.0      1.5   \n",
       "\n",
       "      the slope of the peak exercise ST segment  number of major vessels  tha  \\\n",
       "0                                           2.0                      3.0  3.0   \n",
       "1                                           2.0                      0.0  7.0   \n",
       "2                                           1.0                      0.0  7.0   \n",
       "3                                           2.0                      1.0  7.0   \n",
       "4                                           1.0                      1.0  3.0   \n",
       "..                                          ...                      ...  ...   \n",
       "265                                         1.0                      0.0  7.0   \n",
       "266                                         1.0                      0.0  7.0   \n",
       "267                                         2.0                      0.0  3.0   \n",
       "268                                         2.0                      0.0  6.0   \n",
       "269                                         2.0                      3.0  3.0   \n",
       "\n",
       "     label  \n",
       "0        2  \n",
       "1        1  \n",
       "2        2  \n",
       "3        1  \n",
       "4        1  \n",
       "..     ...  \n",
       "265      1  \n",
       "266      1  \n",
       "267      1  \n",
       "268      1  \n",
       "269      2  \n",
       "\n",
       "[270 rows x 14 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "swiss-gates",
   "metadata": {
    "id": "committed-subcommittee"
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "honest-animation",
   "metadata": {
    "id": "collect-chest"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "trans = StandardScaler()\n",
    "X = trans.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "white-capacity",
   "metadata": {
    "id": "corporate-gardening"
   },
   "outputs": [],
   "source": [
    "label = df.iloc[:, -1]\n",
    "#train_date = pd.read_csv('train_date/train_date.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-provider",
   "metadata": {
    "id": "hazardous-western"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "forty-leadership",
   "metadata": {
    "id": "average-dover"
   },
   "outputs": [],
   "source": [
    "label = label.map({1:0, 2:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "computational-driving",
   "metadata": {
    "id": "thick-module"
   },
   "outputs": [],
   "source": [
    "label = np.array(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-membrane",
   "metadata": {
    "id": "attempted-bunny"
   },
   "source": [
    "## Feature important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "broad-arcade",
   "metadata": {
    "id": "backed-elite"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_mean.fit(X)\n",
    "X_Imputed  = imp_mean.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "informed-prisoner",
   "metadata": {
    "id": "absolute-supervisor"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "feature_names = [f'feature {i}' for i in range(X_Imputed.shape[1])]\n",
    "forest = RandomForestClassifier(random_state=0)\n",
    "forest.fit(X_Imputed, label)\n",
    "importances = forest.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aggressive-subsection",
   "metadata": {
    "id": "unique-suggestion"
   },
   "outputs": [],
   "source": [
    "df_imprtance = pd.DataFrame(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "extended-bowling",
   "metadata": {
    "id": "authorized-clinton"
   },
   "outputs": [],
   "source": [
    "index_top_n_Feature = df_imprtance.sort_values(by=df_imprtance.columns[0],ascending=False).iloc[:12,].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "covered-comparison",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fiscal-literature",
    "outputId": "f31a107e-ddf7-4257-e1d5-db59e1da7a70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12,  2,  7,  9, 11,  4,  0,  3, 10,  8,  1,  6], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_top_n_Feature.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "arabic-airfare",
   "metadata": {
    "id": "imposed-custom"
   },
   "outputs": [],
   "source": [
    "X_Imputed_df = pd.DataFrame(X_Imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "through-importance",
   "metadata": {
    "id": "dimensional-profile"
   },
   "outputs": [],
   "source": [
    "X_Imputed_df = X_Imputed_df.iloc[:,index_top_n_Feature.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-skiing",
   "metadata": {
    "id": "closed-bikini"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "moderate-alcohol",
   "metadata": {
    "id": "institutional-heating"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow\n",
    "\n",
    "def Res_model(X, y, index, epoch):\n",
    "    # define the keras model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(248, input_dim=X.shape[-1], activation='tanh'))\n",
    "    model.add(Dense(64, activation='tanh'))\n",
    "    model.add(Dense(16, activation='tanh'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['mean_absolute_error'])\n",
    "    model.fit(X, y, epochs=epoch, batch_size=5000, verbose = 0)\n",
    "    model.save('DL_model_Heart/model_'+str(index)+'.h5')\n",
    "    return model.predict(X)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "vanilla-prototype",
   "metadata": {
    "id": "arabic-fashion"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "loved-insurance",
   "metadata": {
    "id": "charged-judges"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_Imputed_df, label, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-gnome",
   "metadata": {
    "id": "loose-strategy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "minute-luther",
   "metadata": {
    "id": "historic-emerald",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "import random\n",
    "\n",
    "Res = []\n",
    "predicted_Base =  np.array([0.5] * len(X_train))\n",
    "predicted = predicted_Base\n",
    "\n",
    "for i in range(0, num_Interation):\n",
    "    Res = y_train-predicted\n",
    "    print('Res: ',Res)\n",
    "    Predicted_Res = Res_model(X_train,Res,i)\n",
    "    New_predicted = predicted+ lr*Predicted_Res.reshape((Predicted_Res.shape[0],))\n",
    "    print('New_predicted: ',New_predicted)\n",
    "    predicted = New_predicted\n",
    "    print('============================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "central-front",
   "metadata": {
    "id": "minute-timing"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Plot ROC\n",
    "def roc_plot(y_real, y_predict):    \n",
    "    from sklearn.metrics import roc_curve, auc    \n",
    "    fpr, tpr, thresholds = roc_curve(y_real, y_predict)\n",
    "    auc = auc(fpr, tpr)\n",
    "    #print('Auc: ',auc)\n",
    "    #lw = 2\n",
    "    #plt.figure(1)\n",
    "    #plt.plot([0, 1], [0, 1], 'k--')\n",
    "    #plt.plot(fpr, tpr, label='Sleep disorder (area = {:.3f})'.format(auc))\n",
    "    #plt.plot(fpr, tpr, label='RF (area = {:.3f})'.format(auc))\n",
    "    #plt.xlabel('False positive rate')\n",
    "    #plt.ylabel('True positive rate')\n",
    "    #plt.title('ROC curve')    \n",
    "    #plt.legend(loc='lower right')\n",
    "    #plt.show()\n",
    "    return auc\n",
    "    \n",
    "import math\n",
    "def conf_matrix(y_real, y_predict):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_real, y_predict).ravel()\n",
    "    #print('confusion matrix: (tn, fp, fn, tp)',tn, fp, fn, tp)\n",
    "    #print('acc= ', (tp + tn)/(tp+tn+fp+fn))\n",
    "    #print('pre= ', tp/(tp+fp))\n",
    "    #print('sen= Recall= ', (tp)/(tp+fn))\n",
    "    #print('spec= ', (tn)/(tn+fp))\n",
    "    #print('F1score= ', 2*(tp)/(2*tp+fp+fn))      \n",
    "    #print('mcc= ', ((tp*tn)-(fp*fn))/math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
    "    \n",
    "    \n",
    "    acc= (tp + tn)/(tp+tn+fp+fn)\n",
    "    pre= tp/(tp+fp)\n",
    "    Recall=  (tp)/(tp+fn)\n",
    "    spec= (tn)/(tn+fp)\n",
    "    F1score= (2*(tp)/(2*tp+fp+fn))\n",
    "    mcc= ((tp*tn)-(fp*fn))/math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    \n",
    "    \n",
    "    return tn, fp, fn, tp, acc, pre, Recall, spec, F1score, mcc"
   ]
  },
  {
   "cell_type": "raw",
   "id": "lined-improvement",
   "metadata": {
    "id": "international-keyboard",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "processed-flavor",
   "metadata": {
    "id": "urban-theme"
   },
   "outputs": [],
   "source": [
    "# It can be used to reconstruct the model identically.\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def model_Predict(X, num_Interation):\n",
    "    result = np.array([0.5] * len(X))\n",
    "    for i in range(0, num_Interation):\n",
    "        model = load_model('DL_model_Heart/model_'+str(i)+\".h5\")\n",
    "        RM_predicted = model.predict(X)\n",
    "        result = result+lr*RM_predicted.reshape((len(X), ))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "raw",
   "id": "analyzed-hydrogen",
   "metadata": {
    "id": "employed-celtic"
   },
   "source": [
    "predicted = model_Predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-mercy",
   "metadata": {
    "id": "MHr369yaxkZM"
   },
   "source": [
    "## Running on Multiple parameters"
   ]
  },
  {
   "cell_type": "raw",
   "id": "framed-knock",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "alleged-teddy",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "0104094f-ee40-4298-d5bf-0f351b926a20",
    "tags": []
   },
   "source": [
    "### from imblearn.under_sampling import RandomUnderSampler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# define undersample strategy\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow\n",
    "tensorflow.compat.v1.logging.set_verbosity(tensorflow.compat.v1.logging.ERROR)\n",
    "\n",
    "import timeit\n",
    "import tracemalloc\n",
    "#from imblearn.under_sampling import RandomUnderSampler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "# split 10 fold of undersampling\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve\n",
    "\n",
    "\n",
    "max_auc = 0.9\n",
    "for num_Interation in range(2, 50):\n",
    "    rsFile = \"Result_OverIteration_Bagging_Numeric/NumberOfBag_\"+str(num_Interation)+\".csv\"\n",
    "    f2=open(rsFile,\"w\")\n",
    "    f2.write('num_Interation, epoch, Time_train, Train_Peak_RAM, Time_predict, Peak_Time_prediction, thres, tn, fp, fn, tp, acc, pre, Recall, spec, F1score, mcc, auc\\n')\n",
    "    for epoch in range(10, 500, 10):\n",
    "    \n",
    "        print(\"working on num_Interation: \" + str(num_Interation) + \"_with_epoch_\"+str(epoch))\n",
    "        \n",
    "        Res = []\n",
    "        predicted_Base =  np.array([0.5] * len(X_train))\n",
    "        predicted = predicted_Base\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        start = timeit.default_timer()\n",
    "        tracemalloc.start()\n",
    "\n",
    "        ## Start training\n",
    "        for i in range(0, num_Interation):\n",
    "            Res = y_train-predicted\n",
    "            #print('Res: ',Res)\n",
    "            Predicted_Res = Res_model(X_train,Res, i, epoch)\n",
    "            New_predicted = predicted+ lr*Predicted_Res.reshape((Predicted_Res.shape[0],))\n",
    "            #print('New_predicted: ',New_predicted)\n",
    "            predicted = New_predicted\n",
    "            #print('============================')\n",
    "\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        stop = timeit.default_timer()\n",
    "        Time_train=stop - start\n",
    "        Train_Peak_RAM = peak / 10**6\n",
    "        #print('traning time: ', Time_train)\n",
    "        #print('Peak_Time:', Train_Peak_RAM)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        start = timeit.default_timer()\n",
    "        tracemalloc.start()\n",
    "        \n",
    "        predicted_test = model_Predict(X_test, num_Interation)\n",
    "        \n",
    "        stop = timeit.default_timer()\n",
    "        Time_predict=stop - start\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        Peak_Time_prediction = peak / 10**6\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "        '''result = np.array([[0]*predicted[i].shape[1]]*predicted[i].shape[0])\n",
    "\n",
    "        for i in range(0, num_Interation):\n",
    "            #print(result)\n",
    "            result = result+ predicted[i]\n",
    "            ##print('*=================')'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #print('y_test: ', y_test)\n",
    "        #print('predicted_test: ', predicted_test)\n",
    "\n",
    "        auc=roc_auc_score(y_test, predicted_test)\n",
    "        #print('y_test.shape ' ,y_test.shape)\n",
    "        #print('==========================================')\n",
    "        #print(result[:, 1]/num_Interation)\n",
    "        #print('result.shape ', result.shape)\n",
    "\n",
    "        if max_auc <= auc:\n",
    "            #max_auc = auc\n",
    "            print('===================: ')\n",
    "            print('======= AUC ==============: ', auc)\n",
    "            print('=====AUC===AUC=====: ')\n",
    "            print('===AUC========AUC==: ')\n",
    "            print('=AUC===========AUC=: ')\n",
    "            print('=AUC===========AUC=: ')\n",
    "            print('=AUC===========AUC=: ')\n",
    "            print('===================: ')\n",
    "            print('===================: ')\n",
    "\n",
    "        thres = predicted_test.min()\n",
    "        while (thres <= predicted_test.max()):\n",
    "        #for thres in range(1, 1001):\n",
    "            thres = thres+(predicted_test.max()/500)\n",
    "            yhat = []\n",
    "            for i in range(len(predicted_test)):\n",
    "                if predicted_test[i]>thres:\n",
    "                    yhat.append(1)\n",
    "                else:\n",
    "                    yhat.append(0)\n",
    "\n",
    "            yhat = np.array(yhat)\n",
    "\n",
    "            tn, fp, fn, tp, acc, pre, Recall, spec, F1score, mcc = conf_matrix(y_test, yhat)\n",
    "            #auc = roc_plot(y_test, yhat)\n",
    "            f2.write(str(num_Interation)+\", \"+str(epoch)+\", \"+str(Time_train)+\", \"+str(Train_Peak_RAM)+\", \"+str(Time_predict)+\", \"+str(Peak_Time_prediction)+\", \"+str(thres)+\", \"+str(tn)+\", \"+str(fp)+\", \"+str(fn)+\", \"+str(tp)+\", \"+str(acc)+\", \"+str(pre)+\", \"+str(Recall)+\", \"+str(spec)+\", \"+str(F1score)+\", \"+str(mcc)+\", \"+str(auc)+\"\\n\")\n",
    "\n",
    "\n",
    "    f2.close()\n",
    "    print('WRITING FILE SUCESSFULL ========!!!!!!!!!!!!!!!!!!!!!!')       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-carbon",
   "metadata": {
    "id": "sharp-setup"
   },
   "source": [
    "## Running on CLU 33, epoc 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "representative-textbook",
   "metadata": {
    "id": "harmful-pride"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on num_Interation: 0\n",
      "===================: \n",
      "======= AUC ==============:  0.9027777777777778\n",
      "=====AUC===AUC=====: \n",
      "===AUC========AUC==: \n",
      "=AUC===========AUC=: \n",
      "=AUC===========AUC=: \n",
      "=AUC===========AUC=: \n",
      "===================: \n",
      "===================: \n",
      "WRITING FILE SUCESSFULL ========!!!!!!!!!!!!!!!!!!!!!!\n",
      "working on num_Interation: 1\n",
      "WRITING FILE SUCESSFULL ========!!!!!!!!!!!!!!!!!!!!!!\n",
      "working on num_Interation: 2\n",
      "===================: \n",
      "======= AUC ==============:  0.9185606060606061\n",
      "=====AUC===AUC=====: \n",
      "===AUC========AUC==: \n",
      "=AUC===========AUC=: \n",
      "=AUC===========AUC=: \n",
      "=AUC===========AUC=: \n",
      "===================: \n",
      "===================: \n",
      "WRITING FILE SUCESSFULL ========!!!!!!!!!!!!!!!!!!!!!!\n",
      "working on num_Interation: 3\n",
      "===================: \n",
      "======= AUC ==============:  0.9059343434343435\n",
      "=====AUC===AUC=====: \n",
      "===AUC========AUC==: \n",
      "=AUC===========AUC=: \n",
      "=AUC===========AUC=: \n",
      "=AUC===========AUC=: \n",
      "===================: \n",
      "===================: \n",
      "WRITING FILE SUCESSFULL ========!!!!!!!!!!!!!!!!!!!!!!\n",
      "working on num_Interation: 4\n",
      "===================: \n",
      "======= AUC ==============:  0.9210858585858586\n",
      "=====AUC===AUC=====: \n",
      "===AUC========AUC==: \n",
      "=AUC===========AUC=: \n",
      "=AUC===========AUC=: \n",
      "=AUC===========AUC=: \n",
      "===================: \n",
      "===================: \n",
      "WRITING FILE SUCESSFULL ========!!!!!!!!!!!!!!!!!!!!!!\n",
      "working on num_Interation: 5\n",
      "===================: \n",
      "======= AUC ==============:  0.9147727272727273\n",
      "=====AUC===AUC=====: \n",
      "===AUC========AUC==: \n",
      "=AUC===========AUC=: \n",
      "=AUC===========AUC=: \n",
      "=AUC===========AUC=: \n",
      "===================: \n",
      "===================: \n",
      "WRITING FILE SUCESSFULL ========!!!!!!!!!!!!!!!!!!!!!!\n",
      "working on num_Interation: 6\n",
      "===================: \n",
      "======= AUC ==============:  0.9160353535353536\n",
      "=====AUC===AUC=====: \n",
      "===AUC========AUC==: \n",
      "=AUC===========AUC=: \n",
      "=AUC===========AUC=: \n",
      "=AUC===========AUC=: \n",
      "===================: \n",
      "===================: \n",
      "WRITING FILE SUCESSFULL ========!!!!!!!!!!!!!!!!!!!!!!\n",
      "working on num_Interation: 7\n",
      "===================: \n",
      "======= AUC ==============:  0.9078282828282829\n",
      "=====AUC===AUC=====: \n",
      "===AUC========AUC==: \n",
      "=AUC===========AUC=: \n",
      "=AUC===========AUC=: \n",
      "=AUC===========AUC=: \n",
      "===================: \n",
      "===================: \n",
      "WRITING FILE SUCESSFULL ========!!!!!!!!!!!!!!!!!!!!!!\n",
      "working on num_Interation: 8\n",
      "WRITING FILE SUCESSFULL ========!!!!!!!!!!!!!!!!!!!!!!\n",
      "working on num_Interation: 9\n",
      "===================: \n",
      "======= AUC ==============:  0.9154040404040404\n",
      "=====AUC===AUC=====: \n",
      "===AUC========AUC==: \n",
      "=AUC===========AUC=: \n",
      "=AUC===========AUC=: \n",
      "=AUC===========AUC=: \n",
      "===================: \n",
      "===================: \n",
      "WRITING FILE SUCESSFULL ========!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "### from imblearn.under_sampling import RandomUnderSampler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# define undersample strategy\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow\n",
    "tensorflow.compat.v1.logging.set_verbosity(tensorflow.compat.v1.logging.ERROR)\n",
    "\n",
    "import timeit\n",
    "import tracemalloc\n",
    "#from imblearn.under_sampling import RandomUnderSampler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "# split 10 fold of undersampling\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve\n",
    "\n",
    "\n",
    "max_auc = 0.9\n",
    "\n",
    "for time in range(0, 10):\n",
    "    rsFile = \"Result_OverIteration_Bagging_Numeric/NumberOfIteration\"+str(time)+\".csv\"\n",
    "    f2=open(rsFile,\"w\")\n",
    "    f2.write('num_Interation, epoch, Time_train, Train_Peak_RAM, Time_predict, Peak_Time_prediction, thres, tn, fp, fn, tp, acc, pre, Recall, spec, F1score, mcc, auc\\n')\n",
    "    print(\"working on num_Interation: \" + str(time))\n",
    "    for num_Interation in ([33]): # range(2, 50):\n",
    "        for epoch in ([190]): # range(10, 500, 10):\n",
    "            \n",
    "\n",
    "            Res = []\n",
    "            predicted_Base =  np.array([0.5] * len(X_train))\n",
    "            predicted = predicted_Base\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            start = timeit.default_timer()\n",
    "            tracemalloc.start()\n",
    "\n",
    "            ## Start training\n",
    "            for i in range(0, num_Interation):\n",
    "                Res = y_train-predicted\n",
    "                #print('Res: ',Res)\n",
    "                Predicted_Res = Res_model(X_train,Res, i, epoch)\n",
    "                New_predicted = predicted+ lr*Predicted_Res.reshape((Predicted_Res.shape[0],))\n",
    "                #print('New_predicted: ',New_predicted)\n",
    "                predicted = New_predicted\n",
    "                #print('============================')\n",
    "\n",
    "            current, peak = tracemalloc.get_traced_memory()\n",
    "            stop = timeit.default_timer()\n",
    "            Time_train=stop - start\n",
    "            Train_Peak_RAM = peak / 10**6\n",
    "            #print('traning time: ', Time_train)\n",
    "            #print('Peak_Time:', Train_Peak_RAM)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            start = timeit.default_timer()\n",
    "            tracemalloc.start()\n",
    "\n",
    "            predicted_test = model_Predict(X_test, num_Interation)\n",
    "\n",
    "            stop = timeit.default_timer()\n",
    "            Time_predict=stop - start\n",
    "            current, peak = tracemalloc.get_traced_memory()\n",
    "            Peak_Time_prediction = peak / 10**6\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            '''result = np.array([[0]*predicted[i].shape[1]]*predicted[i].shape[0])\n",
    "\n",
    "            for i in range(0, num_Interation):\n",
    "                #print(result)\n",
    "                result = result+ predicted[i]\n",
    "                ##print('*=================')'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #print('y_test: ', y_test)\n",
    "            #print('predicted_test: ', predicted_test)\n",
    "\n",
    "            auc=roc_auc_score(y_test, predicted_test)\n",
    "            #print('y_test.shape ' ,y_test.shape)\n",
    "            #print('==========================================')\n",
    "            #print(result[:, 1]/num_Interation)\n",
    "            #print('result.shape ', result.shape)\n",
    "\n",
    "            if max_auc <= auc:\n",
    "                #max_auc = auc\n",
    "                print('===================: ')\n",
    "                print('======= AUC ==============: ', auc)\n",
    "                print('=====AUC===AUC=====: ')\n",
    "                print('===AUC========AUC==: ')\n",
    "                print('=AUC===========AUC=: ')\n",
    "                print('=AUC===========AUC=: ')\n",
    "                print('=AUC===========AUC=: ')\n",
    "                print('===================: ')\n",
    "                print('===================: ')\n",
    "\n",
    "            thres = predicted_test.min()\n",
    "            while (thres <= predicted_test.max()):\n",
    "            #for thres in range(1, 1001):\n",
    "                thres = thres+(predicted_test.max()/500)\n",
    "                yhat = []\n",
    "                for i in range(len(predicted_test)):\n",
    "                    if predicted_test[i]>thres:\n",
    "                        yhat.append(1)\n",
    "                    else:\n",
    "                        yhat.append(0)\n",
    "\n",
    "                yhat = np.array(yhat)\n",
    "\n",
    "                tn, fp, fn, tp, acc, pre, Recall, spec, F1score, mcc = conf_matrix(y_test, yhat)\n",
    "                #auc = roc_plot(y_test, yhat)\n",
    "                f2.write(str(num_Interation)+\", \"+str(epoch)+\", \"+str(Time_train)+\", \"+str(Train_Peak_RAM)+\", \"+str(Time_predict)+\", \"+str(Peak_Time_prediction)+\", \"+str(thres)+\", \"+str(tn)+\", \"+str(fp)+\", \"+str(fn)+\", \"+str(tp)+\", \"+str(acc)+\", \"+str(pre)+\", \"+str(Recall)+\", \"+str(spec)+\", \"+str(F1score)+\", \"+str(mcc)+\", \"+str(auc)+\"\\n\")\n",
    "\n",
    "\n",
    "    f2.close()\n",
    "    print('WRITING FILE SUCESSFULL ========!!!!!!!!!!!!!!!!!!!!!!')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-australian",
   "metadata": {
    "id": "stylish-solution"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-debate",
   "metadata": {
    "id": "august-clear"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-bibliography",
   "metadata": {
    "id": "described-starter"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-ordinance",
   "metadata": {
    "id": "attached-shannon"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-enough",
   "metadata": {
    "id": "facial-drama"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-spouse",
   "metadata": {
    "id": "sorted-penalty"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "front-medicine",
   "metadata": {
    "id": "applicable-milan",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "thres = predicted.min()\n",
    "\n",
    "#for thres in range(1, 101):\n",
    "while (thres <= predicted.max()):\n",
    "    thres = thres+0.005\n",
    "    \n",
    "    #print('thres: ', thres/100)\n",
    "    yhat = []\n",
    "    #thres=thres/100\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i]>thres:\n",
    "            yhat.append(1)\n",
    "        else:\n",
    "            yhat.append(0)\n",
    "\n",
    "    yhat = np.array(yhat)\n",
    "\n",
    "    confmat = confusion_matrix(y_true=y_test, y_pred=yhat)\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "    for i in range(confmat.shape[0]):\n",
    "        for j in range(confmat.shape[1]):\n",
    "            ax.text(x=j, y=i, s=confmat[i,j], va='center', ha='center')\n",
    "    plt.xlabel('predicted label')        \n",
    "    plt.ylabel('true label')\n",
    "    plt.show()\n",
    "\n",
    "    conf_matrix(y_test, yhat)\n",
    "    roc_plot(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-measure",
   "metadata": {
    "id": "supported-withdrawal"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-creature",
   "metadata": {
    "id": "cutting-maldives"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-senegal",
   "metadata": {
    "id": "renewable-scottish"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-covering",
   "metadata": {
    "id": "classified-airfare"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-confirmation",
   "metadata": {
    "id": "tight-canadian"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-permit",
   "metadata": {
    "id": "uniform-vector"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-generator",
   "metadata": {
    "id": "material-dutch"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-champion",
   "metadata": {
    "id": "neural-warrant"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-maximum",
   "metadata": {
    "id": "blond-redhead"
   },
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-yield",
   "metadata": {
    "id": "authentic-greenhouse"
   },
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-maple",
   "metadata": {
    "id": "excellent-smith"
   },
   "outputs": [],
   "source": [
    "metrics.roc_curve(y_test, clf.predict(X_test), pos_label=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-festival",
   "metadata": {
    "id": "pretty-float"
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, clf.predict(X_test), pos_label=2)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-peripheral",
   "metadata": {
    "id": "accessory-closer"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "prediction = clf.predict(X_test)\n",
    "\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=prediction)\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confmat.shape[0]):\n",
    "    for j in range(confmat.shape[1]):\n",
    "        ax.text(x=j, y=i, s=confmat[i,j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')        \n",
    "plt.ylabel('true label')\n",
    "plt.show()\n",
    "\n",
    "conf_matrix(y_test, prediction)\n",
    "roc_plot(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-struggle",
   "metadata": {
    "id": "fantastic-congo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-parameter",
   "metadata": {
    "id": "cleared-enterprise"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-burning",
   "metadata": {
    "id": "oriented-sitting"
   },
   "outputs": [],
   "source": [
    "# check xgboost version\n",
    "import xgboost\n",
    "print(xgboost.__version__)\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import mean\n",
    "from numpy import std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-description",
   "metadata": {
    "id": "speaking-glucose"
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = XGBClassifier(scale_pos_weight = len(label[label==0]) / len(label[label==1]), n_estimators=5)\n",
    "#model = XGBClassifier(scale_pos_weight = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-yellow",
   "metadata": {
    "id": "enormous-success"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-homeless",
   "metadata": {
    "id": "sacred-advocate"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-tattoo",
   "metadata": {
    "id": "outdoor-adelaide"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=prediction)\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confmat.shape[0]):\n",
    "    for j in range(confmat.shape[1]):\n",
    "        ax.text(x=j, y=i, s=confmat[i,j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')        \n",
    "plt.ylabel('true label')\n",
    "plt.show()\n",
    "\n",
    "conf_matrix(y_test, prediction)\n",
    "roc_plot(y_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-screening",
   "metadata": {
    "id": "classified-stream"
   },
   "source": [
    "## Using Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-surrey",
   "metadata": {
    "id": "postal-stability"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=13, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-female",
   "metadata": {
    "id": "young-uganda"
   },
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=512, verbose=1)\n",
    "# make class predictions with the model\n",
    "predictions = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-supervision",
   "metadata": {
    "id": "least-sarah"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-house",
   "metadata": {
    "id": "potential-toolbox"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "prediction = predictions.reshape(predictions.shape[0])\n",
    "\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=prediction)\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confmat.shape[0]):\n",
    "    for j in range(confmat.shape[1]):\n",
    "        ax.text(x=j, y=i, s=confmat[i,j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')        \n",
    "plt.ylabel('true label')\n",
    "plt.show()\n",
    "\n",
    "conf_matrix(y_test, prediction)\n",
    "roc_plot(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-hobby",
   "metadata": {
    "id": "informed-treatment"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-arbitration",
   "metadata": {
    "id": "coastal-style"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-square",
   "metadata": {
    "id": "settled-condition"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "applicable-milan",
    "classified-stream"
   ],
   "name": "GBoostWithDL_HeartRate_eVALONePOCH.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

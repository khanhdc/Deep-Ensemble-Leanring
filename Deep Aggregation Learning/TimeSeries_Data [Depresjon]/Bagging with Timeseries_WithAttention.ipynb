{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "incredible-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import timeit\n",
    "import tracemalloc\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.layers import GRU, LSTM\n",
    "from attention import Attention\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stopped-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "file_name_Despress = glob.glob('Depresjon\\condition\\*.csv')\n",
    "file_name_Normal = glob.glob('Depresjon\\control\\*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "contrary-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(file_name):\n",
    "    ID = 0\n",
    "    result = []\n",
    "    for f in file_name:\n",
    "        df = pd.read_csv(f)\n",
    "        out = [x[11:] for x in df.timestamp]df.timestamp = out     \n",
    "        df_Pivot = pd.pivot_table(df,index=[\"timestamp\"],columns=[\"date\"],fill_value=0)\n",
    "        if (df_Pivot.shape[-1]>7):\n",
    "            for j in range(7, df_Pivot.shape[-1]+1):\n",
    "                result.append(df_Pivot.iloc[:, j-7:j])\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "serial-documentary",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Depress = readfile(file_name_Despress)\n",
    "df_Nomal = readfile(file_name_Normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "textile-petersburg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(267, 1440, 7)\n",
      "(547, 1440, 7)\n"
     ]
    }
   ],
   "source": [
    "print(df_Depress.shape)\n",
    "print(df_Nomal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "collaborative-sewing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Final = np.concatenate((df_Depress, df_Nomal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "owned-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "label =[]\n",
    "for i in range(0, df_Depress.shape[0]+df_Nomal.shape[0]):\n",
    "    if i <df_Depress.shape[0]:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "diverse-sector",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(814, 1440, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "roman-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_Final, label, test_size=0.2, random_state=42)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*7)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "directed-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "import math\n",
    "def conf_matrix(y_real, y_predict):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_real, y_predict).ravel()\n",
    "    acc= (tp + tn)/(tp+tn+fp+fn)\n",
    "    pre= tp/(tp+fp)\n",
    "    Recall=  (tp)/(tp+fn)\n",
    "    spec= (tn)/(tn+fp)\n",
    "    F1score= (2*(tp)/(2*tp+fp+fn))\n",
    "    mcc= ((tp*tn)-(fp*fn))/math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    return tn, fp, fn, tp, acc, pre, Recall, spec, F1score, mcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-november",
   "metadata": {},
   "source": [
    "# Using DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "complicated-sponsorship",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# one hot encode\n",
    "y_train = to_categorical(y_train)\n",
    "# one hot encode\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cooperative-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a sample of a scheduler I used in the past\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch % 50 == 0 and epoch:\n",
    "        lr = lr - lr*0.1\n",
    "        return lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "distributed-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(X, y, iteration, epoch):\n",
    "    #X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "    \n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    time_steps = X.shape[1]\n",
    "    input_dim = 1\n",
    "    print((time_steps, input_dim))\n",
    "    \n",
    "    model_path = 'DL_model_Bagging/model_'+str(iteration)+'.h5'\n",
    "    # define the keras model\n",
    "    model_input = Input(shape=(time_steps, input_dim))\n",
    "    x = GRU(128, return_sequences=True)(model_input)\n",
    "    x = Attention(64)(x)\n",
    "    x = Dense(32, activation = \"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(2, activation = \"sigmoid\")(x)\n",
    "    model = Model(model_input, x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    \n",
    "    # train.\n",
    "    model.fit(X, y, epochs=epoch, batch_size=160, validation_split=0.2,verbose=1, \n",
    "                           callbacks = [LearningRateScheduler(lr_scheduler, verbose=0),\n",
    "                                        tensorflow.keras.callbacks.ModelCheckpoint(model_path,\n",
    "                                                                                   monitor='val_loss',\n",
    "                                                                                   save_best_only=True,\n",
    "                                                                                   mode='min',\n",
    "                                                                                   verbose=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "atlantic-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def model_Predict(X):\n",
    "    RM_predicted =[]\n",
    "    for i in range(0, num_Interation):\n",
    "        model = load_model('DL_model_Bagging/model_'+str(i)+\".h5\") \n",
    "        RM_predicted.append(model.predict(X))\n",
    "    return RM_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "noted-ontario",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "import math\n",
    "def conf_matrix(y_real, y_predict):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_real, y_predict).ravel()\n",
    "    acc= (tp + tn)/(tp+tn+fp+fn)\n",
    "    pre= tp/(tp+fp)\n",
    "    Recall=  (tp)/(tp+fn)\n",
    "    spec= (tn)/(tn+fp)\n",
    "    F1score= (2*(tp)/(2*tp+fp+fn))\n",
    "    mcc= ((tp*tn)-(fp*fn))/math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    return tn, fp, fn, tp, acc, pre, Recall, spec, F1score, mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "operational-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import timeit\n",
    "import tracemalloc\n",
    "\n",
    "# define undersample strategy\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# split 10 fold of undersampling\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "max_auc = 0.7\n",
    "for num_Interation in range(20, 50, 5):\n",
    "    rsFile = \"Result_OverIteration_Bagging_Numeric/NumberOfBag_\"+str(num_Interation)+\".csv\"\n",
    "    f2=open(rsFile,\"w\")\n",
    "    f2.write('num_Interation, epoch, Time_train, Train_Peak_RAM, Time_predict, Peak_Time_prediction, thres, tn, fp, fn, tp, acc, pre, Recall, spec, F1score, mcc, auc\\n')\n",
    "    for epoch in (50, 100, 500):\n",
    "\n",
    "        print(\"WORKING ON ITERATION \"+str(num_Interation)+\" epoch: \", str(epoch))\n",
    "        \n",
    "        \n",
    "        \n",
    "        X_train_List=[]\n",
    "        y_train_List = []\n",
    "        for i in range(0, num_Interation):\n",
    "            X_over, y_over = undersample.fit_resample(X_train, np.argmax(y_train, axis=1))\n",
    "            X_train_List.append(X_over)\n",
    "\n",
    "            y_over = to_categorical(y_over)\n",
    "            y_train_List.append(y_over)\n",
    "\n",
    "        X_train_List = np.array(X_train_List)\n",
    "        y_train_List = np.array(y_train_List)\n",
    "\n",
    "        X_train_List = np.array(X_train_List)\n",
    "        y_train_List = np.array(y_train_List)\n",
    "        \n",
    "        \n",
    "        # ============== Starting training\n",
    "        start = timeit.default_timer()\n",
    "        tracemalloc.start()\n",
    "        \n",
    "        ## Start training\n",
    "        for i in range(0, num_Interation):\n",
    "            #X_train_List[i] = X_train_List[i].reshape(X_train_List[i].shape[0], X_train_List[i].shape[1], 1)\n",
    "            training(X_train_List[i].reshape(X_train_List[i].shape[0], X_train_List[i].shape[1], 1), y_train_List[i] , i, epoch)\n",
    "        \n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        stop = timeit.default_timer()\n",
    "        Time_train=stop - start\n",
    "        Train_Peak_RAM = peak / 10**6\n",
    "        \n",
    "        \n",
    "        start = timeit.default_timer()\n",
    "        tracemalloc.start()\n",
    "\n",
    "        predicted = model_Predict(X_test)\n",
    "\n",
    "        stop = timeit.default_timer()\n",
    "        Time_predict=stop - start\n",
    "        #thres = predicted.min()\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        Peak_Time_prediction = peak / 10**6\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(0, num_Interation):\n",
    "            result = np.array([[0]*predicted[i].shape[1]]*predicted[i].shape[0])\n",
    "\n",
    "        for i in range(0, num_Interation):\n",
    "            #print(result)\n",
    "            result = result+ predicted[i]\n",
    "            ##print('*=================')\n",
    "\n",
    "\n",
    "\n",
    "        #print('result: ', result)\n",
    "\n",
    "        auc=roc_auc_score(np.argmax(y_test, axis = 1), result[:, 1]/num_Interation)\n",
    "        \n",
    "\n",
    "        if max_auc <= auc:\n",
    "            #max_auc = auc\n",
    "            print('===================: ')\n",
    "            print('======= AUC ==============: ', auc)\n",
    "            print('=====AUC===AUC=====: ')\n",
    "            print('===AUC========AUC==: ')\n",
    "            print('=AUC===========AUC=: ')\n",
    "            print('=AUC===========AUC=: ')\n",
    "            print('=AUC===========AUC=: ')\n",
    "            print('===================: ')\n",
    "            print('===================: ')\n",
    "\n",
    "\n",
    "\n",
    "        thres = result.min()\n",
    "        while (thres <= result.max()):\n",
    "        #for thres in range(1, 1001):\n",
    "            thres = thres+(result.max()/300)\n",
    "            yhat = []\n",
    "            for i in range(len(result)):\n",
    "                if result[i][0]>thres:\n",
    "                    yhat.append(0)\n",
    "                else:\n",
    "                    yhat.append(1)\n",
    "\n",
    "            yhat = np.array(yhat)\n",
    "\n",
    "            tn, fp, fn, tp, acc, pre, Recall, spec, F1score, mcc = conf_matrix(np.argmax(y_test, axis = 1), yhat)\n",
    "            #auc = roc_plot(y_test, yhat)\n",
    "            f2.write(str(num_Interation)+\", \"+str(epoch)+\", \"+str(Time_train)+\", \"+str(Train_Peak_RAM)+\", \"+str(Time_predict)+\", \"+str(Peak_Time_prediction)+\", \"+str(thres)+\", \"+str(tn)+\", \"+str(fp)+\", \"+str(fn)+\", \"+str(tp)+\", \"+str(acc)+\", \"+str(pre)+\", \"+str(Recall)+\", \"+str(spec)+\", \"+str(F1score)+\", \"+str(mcc)+\", \"+str(auc)+\"\\n\")\n",
    "\n",
    "\n",
    "    f2.close()\n",
    "    print('WRITING FILE SUCESSFULL ========!!!!!!!!!!!!!!!!!!!!!!')       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

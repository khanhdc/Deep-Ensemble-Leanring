{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beautiful-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "# example of zoom image augmentation\n",
    "from numpy import expand_dims\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "finite-reservoir",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = R\"D:\\Imbalanced Dataset\\chest_xray\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "confident-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(main_path,\"train\")\n",
    "test_path=os.path.join(main_path,\"test\")\n",
    "val_path=os.path.join(main_path,\"val\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "joined-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_train_images = glob.glob(train_path+\"\\\\PNEUMONIA\\\\*.jpeg\")\n",
    "normal_train_images = glob.glob(train_path+\"\\\\NORMAL\\\\*.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "thermal-walnut",
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_test_images = glob.glob(test_path+\"\\\\PNEUMONIA\\\\*.jpeg\")\n",
    "normal_test_images = glob.glob(test_path+\"\\\\NORMAL\\\\*.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "graphic-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_val_images = glob.glob(val_path+\"\\\\PNEUMONIA\\\\*.jpeg\")\n",
    "normal_val_images = glob.glob(val_path+\"\\\\NORMAL\\\\*.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "center-filing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1341"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normal_train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "medical-manual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3875"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pneumonia_train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "funky-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(np.concatenate([[0]*len(normal_train_images) , [1] *  len(pneumonia_train_images)]),columns=[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-signal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "seeing-private",
   "metadata": {},
   "source": [
    "# Defining Image Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-candy",
   "metadata": {},
   "source": [
    "## Load image into array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "million-mineral",
   "metadata": {},
   "outputs": [],
   "source": [
    "Normal_train=[]\n",
    "pneumonia_trian =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "complicated-needle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1341/1341 [00:36<00:00, 37.12it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tqdm import tqdm\n",
    "for dirs in tqdm(normal_train_images):\n",
    "    img = cv2.imread(dirs)    \n",
    "    img2gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    resized = cv2.resize(img2gray, (200,200))\n",
    "    #print(resized.shape)\n",
    "    Normal_train.append(np.array(resized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "increasing-rider",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3875/3875 [00:37<00:00, 102.26it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tqdm import tqdm\n",
    "for dirs in tqdm(pneumonia_train_images):\n",
    "    img = cv2.imread(dirs)    \n",
    "    img2gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    resized = cv2.resize(img2gray, (200,200))\n",
    "    #print(resized.shape)\n",
    "    pneumonia_trian.append(np.array(resized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "classified-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "danish-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "Normal_train_array=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "floppy-whale",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1341/1341 [00:00<00:00, 1343660.22it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0,len(Normal_train))):\n",
    "    #print(a.shape)\n",
    "    #print(Normal_train[i].shape)\n",
    "    Normal_train_array.append(Normal_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "appropriate-evening",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1341/1341 [11:01<00:00,  2.03it/s]\n"
     ]
    }
   ],
   "source": [
    "ind = 0\n",
    "from tqdm import tqdm\n",
    "for dirs in tqdm(normal_train_images):\n",
    "    ind=ind+1\n",
    "    img = cv2.imread(dirs) \n",
    "    data = img_to_array(img)\n",
    "    samples = expand_dims(data, 0)\n",
    "    \n",
    "    if ind%2 ==0:\n",
    "        continue\n",
    "    else:\n",
    "        datagen = ImageDataGenerator(zoom_range=[0.8,1.0])\n",
    "        it = datagen.flow(samples, batch_size=1)\n",
    "        for i in range(2):\n",
    "            batch = it.next()\n",
    "            image = batch[0].astype('uint8')\n",
    "            resized = cv2.resize(image, (200,200))\n",
    "            img2gray = cv2.cvtColor(resized, cv2.COLOR_RGB2GRAY)\n",
    "            Normal_train_array.append(img2gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "unlikely-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "Normal_train_array_X = np.array(Normal_train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "metropolitan-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "Normal_train_array_y = [0]*len(Normal_train_array_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-damages",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "minimal-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_trian_array_X = np.array(pneumonia_trian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "biological-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_train_array_y = [1]*len(pneumonia_trian_array_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "coral-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "raw",
   "id": "mysterious-computer",
   "metadata": {},
   "source": [
    "X_train_List=[]\n",
    "y_train_List = []"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cutting-rebate",
   "metadata": {},
   "source": [
    "for i in range(0, num_Interation):\n",
    "    pneumonia = np.array(random.sample(pneumonia_trian, len(Normal_train_array)))\n",
    "    normal = np.array(Normal_train_array)\n",
    "    X_train_List.append(np.concatenate((normal, pneumonia)))\n",
    "    y_train_List.append(np.concatenate((([0]*len(Normal_train_array_X), [1]*len(Normal_train_array_X)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-joshua",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "regular-round",
   "metadata": {},
   "source": [
    "### Read testing Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "textile-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "Normal_test=[]\n",
    "pneumonia_test =[]\n",
    "Normal_test_array=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "secret-enhancement",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:04<00:00, 48.69it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tqdm import tqdm\n",
    "for dirs in tqdm(normal_test_images):\n",
    "    img = cv2.imread(dirs)    \n",
    "    img2gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    resized = cv2.resize(img2gray, (200,200))\n",
    "    #print(resized.shape)\n",
    "    Normal_test.append(np.array(resized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "lovely-hormone",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 390/390 [00:03<00:00, 129.61it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tqdm import tqdm\n",
    "for dirs in tqdm(pneumonia_test_images):\n",
    "    img = cv2.imread(dirs)    \n",
    "    img2gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    resized = cv2.resize(img2gray, (200,200))\n",
    "    #print(resized.shape)\n",
    "    pneumonia_test.append(np.array(resized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "younger-denver",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:00<00:00, 234520.22it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0,len(Normal_test))):\n",
    "    #print(a.shape)\n",
    "    #print(Normal_train[i].shape)\n",
    "    Normal_test_array.append(Normal_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "south-tulsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Normal_test_array_X = np.array(Normal_test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "absolute-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "Normal_test_array_y = [0]*len(Normal_test_array_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "primary-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_test_array_X = np.array(pneumonia_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adopted-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_test_array_y = [1]*len(pneumonia_test_array_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "pacific-reducing",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.concatenate((Normal_test_array_X, pneumonia_test_array_X), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "pediatric-concentrate",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate((np.array(Normal_test_array_y), np.array(pneumonia_test_array_y)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "valuable-energy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(624,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "successful-wiring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(624, 200, 200)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-parish",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-manchester",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "worth-breath",
   "metadata": {},
   "source": [
    "# Start fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "transparent-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# This is a sample of a scheduler I used in the past\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch % 100 == 0 and epoch:\n",
    "        lr = lr - lr*0.1\n",
    "        return lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "raw",
   "id": "seventh-measure",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "iraqi-forge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of loading the vgg16 model\n",
    "from tensorflow.keras.applications import ResNet50V2, DenseNet121\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense\n",
    "import tensorflow\n",
    "\n",
    "def training(X, y, index, epoch):\n",
    "    # define the keras model\n",
    "    model_path = 'DL_model_Bagging/model_'+str(index)+'.h5'\n",
    "    \n",
    "    X = X.reshape(X.shape[0],X.shape[1], X.shape[2], 1)\n",
    "    \n",
    "    model = ResNet50V2(include_top=False, weights=None, input_tensor=Input(shape=((X.shape[1], X.shape[2], 1))))\n",
    "    \n",
    "    x = model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation = 'sigmoid')(x)\n",
    "    x = Dense(2, activation = \"sigmoid\")(x)\n",
    "    model = tensorflow.keras.Model(model.input, x)\n",
    "    # summarize the model\n",
    "    #model.summary()   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X, y, epochs=epoch, batch_size=120, validation_split=0.2,verbose=1, \n",
    "                       callbacks = [LearningRateScheduler(lr_scheduler, verbose=0),\n",
    "                                    tensorflow.keras.callbacks.ModelCheckpoint(model_path,\n",
    "                                                                               monitor='val_loss',\n",
    "                                                                               save_best_only=True,\n",
    "                                                                               mode='min',\n",
    "                                                                               verbose=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "collective-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    \n",
    "import math\n",
    "def conf_matrix(y_real, y_predict):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_real, y_predict).ravel()\n",
    "\n",
    "    #print('confusion matrix: (tn, fp, fn, tp)',tn, fp, fn, tp)\n",
    "    acc= (tp + tn)/(tp+tn+fp+fn)\n",
    "    pre= tp/(tp+fp)\n",
    "    sen= (tp)/(tp+fn)\n",
    "    spec= (tn)/(tn+fp)\n",
    "    F1score= 2*(tp)/(2*tp+fp+fn)  \n",
    "    mcc= ((tp*tn)-(fp*fn))/math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "\n",
    "    return tn, fp, fn, tp, acc, pre, sen, spec, F1score, mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fitting-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "def one_hot(y):\n",
    "    return to_categorical(y)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "polyphonic-feature",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Start training\n",
    "for i in range(0, num_Interation):\n",
    "    training(X_train_List[i], one_hot(y_train_List[i]), i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "controlling-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def model_Predict(X):\n",
    "    try:\n",
    "        del(model)\n",
    "    except:\n",
    "        print(\"Oops!  Nomodel to delete...\")\n",
    "    \n",
    "    \n",
    "   \n",
    "    X = X.reshape(X.shape[0],X.shape[1], X.shape[2], 1)\n",
    "    \n",
    "    RM_predicted =[]\n",
    "    for i in range(0, num_Interation):\n",
    "        model = load_model('DL_model_Bagging/model_'+str(i)+\".h5\")\n",
    "        RM_predicted.append(model.predict(X))\n",
    "    \n",
    "        \n",
    "    return RM_predicted"
   ]
  },
  {
   "cell_type": "raw",
   "id": "saved-behalf",
   "metadata": {},
   "source": [
    "predicted = model_Predict(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "forward-saint",
   "metadata": {},
   "source": [
    "np.array(predicted).shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "skilled-resource",
   "metadata": {},
   "source": [
    "result = np.array([[0]*predicted[i].shape[1]]*predicted[i].shape[0])\n",
    "for i in range(0, num_Interation):\n",
    "    result = result+ predicted[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-panama",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-muscle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "inner-guarantee",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# define undersample strategy\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# split 10 fold of undersampling\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "\n",
    "max_auc = 0.88\n",
    "for num_Interation in range(10, 50, 5):\n",
    "    rsFile = \"Result_OverIteration_Bagging_Numeric/Image/NumberOfBag_\"+str(num_Interation)+\".csv\"\n",
    "    f2=open(rsFile,\"w\")\n",
    "    f2.write('num_Interation, epoch, thres, tn, fp, fn, tp, acc, pre, Recall, spec, F1score, mcc, auc\\n')\n",
    "    for epoch in (100, 500):\n",
    "        \n",
    "        \n",
    "        print(\"working on num_Interation: \"+str(num_Interation) +\"With epoch: \"+str(epoch) )\n",
    "        X_train_List=[]\n",
    "        y_train_List = []\n",
    "\n",
    "        #Generating the bags\n",
    "\n",
    "        for i in range(0, num_Interation):\n",
    "            #print(len(Normal_train_array)*1.3)\n",
    "            pneumonia = np.array(random.sample(pneumonia_trian, int(len(Normal_train_array)*1.41)))\n",
    "            normal = np.array(Normal_train_array)\n",
    "            X_train_List.append(np.concatenate((normal, pneumonia)))\n",
    "            y_train_List.append(np.concatenate((([0]*len(Normal_train_array_X), [1]*(len(Normal_train_array_X)*2)))))\n",
    "\n",
    "        X_train_List = np.array(X_train_List)\n",
    "        y_train_List = np.array(y_train_List)\n",
    "\n",
    "        X_train_List = np.array(X_train_List)\n",
    "        y_train_List = np.array(y_train_List)\n",
    "\n",
    "\n",
    "        ## Start training\n",
    "        for i in range(0, num_Interation):\n",
    "            training(X_train_List[i], to_categorical(y_train_List[i]) , i, epoch)\n",
    "\n",
    "        predicted = model_Predict(X_test)\n",
    "        result = np.array([[0]*predicted[i].shape[1]]*predicted[i].shape[0])\n",
    "\n",
    "        for i in range(0, num_Interation):\n",
    "            #print(result)\n",
    "            result = result+ predicted[i]\n",
    "            ##print('*=================')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #print(y_test)\n",
    "        #print(len(result[:, 1]))\n",
    "\n",
    "\n",
    "        auc=roc_auc_score(y_test, result[:, 1]/num_Interation)\n",
    "        #print(np.argmax(y_test, axis=1))\n",
    "        print('==========================================')\n",
    "        #print(result[:, 1]/num_Interation)\n",
    "        #print(result[:, 1].shape)\n",
    "\n",
    "        if max_auc <= auc:\n",
    "            max_auc = auc\n",
    "            print('======= auc =======: ', auc)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        print(\"result: \", result)\n",
    "        print(\"result[:, 1].min() : \", result[:, 1].min())\n",
    "        print(\"result[:, 1].max() : \", result[:, 1].max())\n",
    "        thres = result[:, 1].min()\n",
    "        while (thres <= result[:, 1].max()):\n",
    "        #for thres in range(1, 1001):\n",
    "            thres = thres+(result[:, 1].max()/500)\n",
    "            yhat = []\n",
    "            for i in range(len(result)):\n",
    "                if result[i][1]>thres:\n",
    "                    yhat.append(1)\n",
    "                else:\n",
    "                    yhat.append(0)\n",
    "\n",
    "            yhat = np.array(yhat)\n",
    "\n",
    "            tn, fp, fn, tp, acc, pre, Recall, spec, F1score, mcc = conf_matrix(y_test, yhat)\n",
    "            #auc = roc_plot(np.argmax(y_test, axis=1), yhat)\n",
    "            f2.write(str(num_Interation)+\", \"+str(epoch)+\", \"+str(thres)+\", \"+str(tn)+\", \"+str(fp)+\", \"+str(fn)+\", \"+str(tp)+\", \"+str(acc)+\", \"+str(pre)+\", \"+str(Recall)+\", \"+str(spec)+\", \"+str(F1score)+\", \"+str(mcc)+\", \"+str(auc)+\"\\n\")\n",
    "\n",
    "    \n",
    "    f2.close()\n",
    "    print('WRITING FILE SUCESSFULL ========!!!!!!!!!!!!!!!!!!!!!!')       "
   ]
  },
  {
   "cell_type": "raw",
   "id": "classified-contemporary",
   "metadata": {},
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# define undersample strategy\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# split 10 fold of undersampling\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "\n",
    "max_auc = 0.88\n",
    "\n",
    "for time in range(0, 10):\n",
    "    rsFile = \"Result_OverIteration_Bagging_Numeric/Image/Running OnIteration\"+str(time)+\".csv\"\n",
    "    f2=open(rsFile,\"w\")\n",
    "    f2.write('num_Interation, epoch, thres, tn, fp, fn, tp, acc, pre, Recall, spec, F1score, mcc, auc\\n')\n",
    "    for num_Interation in ([15]):\n",
    "        \n",
    "        for epoch in ([150]):\n",
    "\n",
    "\n",
    "            print(\"working on num_Interation: \"+str(num_Interation) +\"With epoch: \"+str(epoch) )\n",
    "            X_train_List=[]\n",
    "            y_train_List = []\n",
    "\n",
    "            #Generating the bags\n",
    "\n",
    "            for i in range(0, num_Interation):\n",
    "                #print(len(Normal_train_array)*1.3)\n",
    "                pneumonia = np.array(random.sample(pneumonia_trian, int(len(Normal_train_array)*1.41)))\n",
    "                normal = np.array(Normal_train_array)\n",
    "                X_train_List.append(np.concatenate((normal, pneumonia)))\n",
    "                y_train_List.append(np.concatenate((([0]*len(Normal_train_array_X), [1]*(len(Normal_train_array_X)*2)))))\n",
    "\n",
    "            X_train_List = np.array(X_train_List)\n",
    "            y_train_List = np.array(y_train_List)\n",
    "\n",
    "            X_train_List = np.array(X_train_List)\n",
    "            y_train_List = np.array(y_train_List)\n",
    "\n",
    "\n",
    "            ## Start training\n",
    "            for i in range(0, num_Interation):\n",
    "                training(X_train_List[i], to_categorical(y_train_List[i]) , i, epoch)\n",
    "\n",
    "            predicted = model_Predict(X_test)\n",
    "            result = np.array([[0]*predicted[i].shape[1]]*predicted[i].shape[0])\n",
    "\n",
    "            for i in range(0, num_Interation):\n",
    "                #print(result)\n",
    "                result = result+ predicted[i]\n",
    "                ##print('*=================')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #print(y_test)\n",
    "            #print(len(result[:, 1]))\n",
    "\n",
    "\n",
    "            auc=roc_auc_score(y_test, result[:, 1]/num_Interation)\n",
    "            #print(np.argmax(y_test, axis=1))\n",
    "            print('==========================================')\n",
    "            #print(result[:, 1]/num_Interation)\n",
    "            #print(result[:, 1].shape)\n",
    "\n",
    "            if max_auc <= auc:\n",
    "                max_auc = auc\n",
    "                print('======= auc =======: ', auc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            print(\"result: \", result)\n",
    "            print(\"result[:, 1].min() : \", result[:, 1].min())\n",
    "            print(\"result[:, 1].max() : \", result[:, 1].max())\n",
    "            thres = result[:, 1].min()\n",
    "            while (thres <= result[:, 1].max()):\n",
    "            #for thres in range(1, 1001):\n",
    "                thres = thres+(result[:, 1].max()/500)\n",
    "                yhat = []\n",
    "                for i in range(len(result)):\n",
    "                    if result[i][1]>thres:\n",
    "                        yhat.append(1)\n",
    "                    else:\n",
    "                        yhat.append(0)\n",
    "\n",
    "                yhat = np.array(yhat)\n",
    "\n",
    "                tn, fp, fn, tp, acc, pre, Recall, spec, F1score, mcc = conf_matrix(y_test, yhat)\n",
    "                #auc = roc_plot(np.argmax(y_test, axis=1), yhat)\n",
    "                f2.write(str(num_Interation)+\", \"+str(epoch)+\", \"+str(thres)+\", \"+str(tn)+\", \"+str(fp)+\", \"+str(fn)+\", \"+str(tp)+\", \"+str(acc)+\", \"+str(pre)+\", \"+str(Recall)+\", \"+str(spec)+\", \"+str(F1score)+\", \"+str(mcc)+\", \"+str(auc)+\"\\n\")\n",
    "\n",
    "\n",
    "    f2.close()\n",
    "    print('WRITING FILE SUCESSFULL ========!!!!!!!!!!!!!!!!!!!!!!')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-update",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "supposed-uniform",
   "metadata": {},
   "source": [
    "## Checking on Time Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "greater-assignment",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-b11bf49ac5bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train_List\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_List' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_List[i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dried-season",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train_List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-52d0c8c043ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_List\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train_List' is not defined"
     ]
    }
   ],
   "source": [
    "to_categorical(y_train_List[i]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "based-error",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on num_Interation: 15With epoch: 10\n",
      "Train on 5172 samples, validate on 1294 samples\n",
      "Epoch 1/10\n",
      "5172/5172 [==============================] - 54s 10ms/sample - loss: 0.4482 - accuracy: 0.8406 - val_loss: 1.7334 - val_accuracy: 0.0054\n",
      "Epoch 2/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.2505 - accuracy: 0.9319 - val_loss: 0.3150 - val_accuracy: 0.9324\n",
      "Epoch 3/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.2096 - accuracy: 0.9414 - val_loss: 0.4078 - val_accuracy: 0.8211\n",
      "Epoch 4/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1576 - accuracy: 0.9569 - val_loss: 2.4887 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1560 - accuracy: 0.9564 - val_loss: 2.0789 - val_accuracy: 0.0641\n",
      "Epoch 6/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1344 - accuracy: 0.9592 - val_loss: 2.0780 - val_accuracy: 0.2689\n",
      "Epoch 7/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1792 - accuracy: 0.9413 - val_loss: 2.7897 - val_accuracy: 0.0193\n",
      "Epoch 8/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.1413 - accuracy: 0.9521 - val_loss: 0.1436 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1274 - accuracy: 0.9588 - val_loss: 3.5417 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1325 - accuracy: 0.9570 - val_loss: 0.2913 - val_accuracy: 0.9281\n",
      "Train on 5172 samples, validate on 1294 samples\n",
      "Epoch 1/10\n",
      "5172/5172 [==============================] - 46s 9ms/sample - loss: 1.1548 - accuracy: 0.4824 - val_loss: 0.3110 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.8365 - accuracy: 0.4892 - val_loss: 0.5331 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.7172 - accuracy: 0.5000 - val_loss: 0.6627 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.6942 - accuracy: 0.5027 - val_loss: 0.7146 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.6929 - accuracy: 0.5162 - val_loss: 0.7359 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.6925 - accuracy: 0.5188 - val_loss: 0.7399 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.6925 - accuracy: 0.5188 - val_loss: 0.7261 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.6925 - accuracy: 0.5188 - val_loss: 0.7349 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.6925 - accuracy: 0.5188 - val_loss: 0.7257 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.6925 - accuracy: 0.5188 - val_loss: 0.7380 - val_accuracy: 0.0000e+00\n",
      "Train on 5172 samples, validate on 1294 samples\n",
      "Epoch 1/10\n",
      "5172/5172 [==============================] - 46s 9ms/sample - loss: 0.9490 - accuracy: 0.6785 - val_loss: 1.8912 - val_accuracy: 3.8640e-04\n",
      "Epoch 2/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.6303 - accuracy: 0.7170 - val_loss: 1.4475 - val_accuracy: 0.0487\n",
      "Epoch 3/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.4716 - accuracy: 0.7189 - val_loss: 1.0317 - val_accuracy: 0.1221\n",
      "Epoch 4/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.3985 - accuracy: 0.8831 - val_loss: 1.5707 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.3343 - accuracy: 0.9278 - val_loss: 1.2987 - val_accuracy: 0.1059\n",
      "Epoch 6/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.2958 - accuracy: 0.9361 - val_loss: 1.7823 - val_accuracy: 0.0100\n",
      "Epoch 7/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.2725 - accuracy: 0.9372 - val_loss: 1.5757 - val_accuracy: 0.0781\n",
      "Epoch 8/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.2429 - accuracy: 0.9470 - val_loss: 2.2054 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.2435 - accuracy: 0.9351 - val_loss: 2.3052 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.2060 - accuracy: 0.9515 - val_loss: 2.3692 - val_accuracy: 0.0023\n",
      "Train on 5172 samples, validate on 1294 samples\n",
      "Epoch 1/10\n",
      "5172/5172 [==============================] - 46s 9ms/sample - loss: 1.0654 - accuracy: 0.5153 - val_loss: 1.6139 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.7930 - accuracy: 0.6319 - val_loss: 1.1840 - val_accuracy: 0.0174\n",
      "Epoch 3/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.6439 - accuracy: 0.7106 - val_loss: 0.8864 - val_accuracy: 0.3238\n",
      "Epoch 4/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.5784 - accuracy: 0.7109 - val_loss: 0.6937 - val_accuracy: 0.3717\n",
      "Epoch 5/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.5464 - accuracy: 0.7495 - val_loss: 0.6775 - val_accuracy: 0.4861\n",
      "Epoch 6/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.5085 - accuracy: 0.9079 - val_loss: 0.7748 - val_accuracy: 0.4486\n",
      "Epoch 7/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.4896 - accuracy: 0.9060 - val_loss: 0.5624 - val_accuracy: 0.8412\n",
      "Epoch 8/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.4641 - accuracy: 0.9128 - val_loss: 0.7040 - val_accuracy: 0.5054\n",
      "Epoch 9/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.4375 - accuracy: 0.9177 - val_loss: 0.8393 - val_accuracy: 0.2639\n",
      "Epoch 10/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.4077 - accuracy: 0.9202 - val_loss: 0.3175 - val_accuracy: 1.0000\n",
      "Train on 5172 samples, validate on 1294 samples\n",
      "Epoch 1/10\n",
      "5172/5172 [==============================] - 45s 9ms/sample - loss: 0.6201 - accuracy: 0.6210 - val_loss: 1.4912 - val_accuracy: 0.4950\n",
      "Epoch 2/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.3302 - accuracy: 0.9216 - val_loss: 1.0832 - val_accuracy: 0.3856\n",
      "Epoch 3/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.2668 - accuracy: 0.9319 - val_loss: 2.0046 - val_accuracy: 0.0209\n",
      "Epoch 4/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.2224 - accuracy: 0.9451 - val_loss: 0.1074 - val_accuracy: 0.9985\n",
      "Epoch 5/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1978 - accuracy: 0.9487 - val_loss: 1.9442 - val_accuracy: 0.0618\n",
      "Epoch 6/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1752 - accuracy: 0.9563 - val_loss: 2.5756 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.2334 - accuracy: 0.9179 - val_loss: 2.7034 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1657 - accuracy: 0.9524 - val_loss: 2.2961 - val_accuracy: 0.0321\n",
      "Epoch 9/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1852 - accuracy: 0.9402 - val_loss: 1.1233 - val_accuracy: 0.3033\n",
      "Epoch 10/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1629 - accuracy: 0.9516 - val_loss: 1.7971 - val_accuracy: 0.1839\n",
      "Train on 5172 samples, validate on 1294 samples\n",
      "Epoch 1/10\n",
      "5172/5172 [==============================] - 45s 9ms/sample - loss: 1.1388 - accuracy: 0.6482 - val_loss: 0.5936 - val_accuracy: 0.5008\n",
      "Epoch 2/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.6397 - accuracy: 0.7717 - val_loss: 1.7137 - val_accuracy: 0.0464\n",
      "Epoch 3/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.2510 - accuracy: 0.9201 - val_loss: 2.3713 - val_accuracy: 0.0468\n",
      "Epoch 4/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.1792 - accuracy: 0.9506 - val_loss: 0.1827 - val_accuracy: 0.9594\n",
      "Epoch 5/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.1733 - accuracy: 0.9415 - val_loss: 0.0856 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1422 - accuracy: 0.9551 - val_loss: 1.1701 - val_accuracy: 0.4347\n",
      "Epoch 7/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1268 - accuracy: 0.9627 - val_loss: 0.9036 - val_accuracy: 0.5788\n",
      "Epoch 8/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1227 - accuracy: 0.9606 - val_loss: 0.1578 - val_accuracy: 0.9745\n",
      "Epoch 9/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.1184 - accuracy: 0.9629 - val_loss: 0.0337 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1015 - accuracy: 0.9701 - val_loss: 1.5785 - val_accuracy: 0.2175\n",
      "Train on 5172 samples, validate on 1294 samples\n",
      "Epoch 1/10\n",
      "5172/5172 [==============================] - 45s 9ms/sample - loss: 0.4823 - accuracy: 0.7589 - val_loss: 1.4304 - val_accuracy: 3.8640e-04\n",
      "Epoch 2/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.2317 - accuracy: 0.9262 - val_loss: 0.1238 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.2078 - accuracy: 0.9352 - val_loss: 0.5413 - val_accuracy: 0.8006\n",
      "Epoch 4/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1974 - accuracy: 0.9407 - val_loss: 0.2197 - val_accuracy: 0.9930\n",
      "Epoch 5/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.1627 - accuracy: 0.9510 - val_loss: 0.0590 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1601 - accuracy: 0.9461 - val_loss: 0.0591 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1509 - accuracy: 0.9503 - val_loss: 0.0680 - val_accuracy: 0.9845\n",
      "Epoch 8/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1304 - accuracy: 0.9592 - val_loss: 3.1390 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.1320 - accuracy: 0.9563 - val_loss: 0.0309 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1178 - accuracy: 0.9628 - val_loss: 2.7353 - val_accuracy: 0.0112\n",
      "Train on 5172 samples, validate on 1294 samples\n",
      "Epoch 1/10\n",
      "5172/5172 [==============================] - 45s 9ms/sample - loss: 0.8434 - accuracy: 0.6590 - val_loss: 1.9099 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.3143 - accuracy: 0.9116 - val_loss: 1.3183 - val_accuracy: 0.1522\n",
      "Epoch 3/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.2329 - accuracy: 0.9265 - val_loss: 2.3927 - val_accuracy: 0.0035\n",
      "Epoch 4/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.1902 - accuracy: 0.9399 - val_loss: 1.0061 - val_accuracy: 0.4625\n",
      "Epoch 5/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1718 - accuracy: 0.9460 - val_loss: 2.3111 - val_accuracy: 0.0487\n",
      "Epoch 6/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1733 - accuracy: 0.9467 - val_loss: 1.2869 - val_accuracy: 0.4274\n",
      "Epoch 7/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1300 - accuracy: 0.9637 - val_loss: 2.6316 - val_accuracy: 0.0015\n",
      "Epoch 8/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1307 - accuracy: 0.9611 - val_loss: 1.4475 - val_accuracy: 0.2257\n",
      "Epoch 9/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.1509 - accuracy: 0.9496 - val_loss: 0.3718 - val_accuracy: 0.8609\n",
      "Epoch 10/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1179 - accuracy: 0.9644 - val_loss: 2.7430 - val_accuracy: 0.0638\n",
      "Train on 5172 samples, validate on 1294 samples\n",
      "Epoch 1/10\n",
      "5172/5172 [==============================] - 46s 9ms/sample - loss: 0.9833 - accuracy: 0.5157 - val_loss: 1.3421 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.7319 - accuracy: 0.5188 - val_loss: 0.8263 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.6937 - accuracy: 0.5188 - val_loss: 0.7340 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.6927 - accuracy: 0.5188 - val_loss: 0.7283 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.6926 - accuracy: 0.5188 - val_loss: 0.7326 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.6926 - accuracy: 0.5188 - val_loss: 0.7384 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.6926 - accuracy: 0.5188 - val_loss: 0.7395 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.6926 - accuracy: 0.5188 - val_loss: 0.7354 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.6925 - accuracy: 0.5188 - val_loss: 0.7462 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.6929 - accuracy: 0.5188 - val_loss: 0.7413 - val_accuracy: 0.0000e+00\n",
      "Train on 5172 samples, validate on 1294 samples\n",
      "Epoch 1/10\n",
      "5172/5172 [==============================] - 46s 9ms/sample - loss: 0.6174 - accuracy: 0.7753 - val_loss: 2.2450 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.2721 - accuracy: 0.9129 - val_loss: 1.6478 - val_accuracy: 0.0997\n",
      "Epoch 3/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.2176 - accuracy: 0.9298 - val_loss: 1.9440 - val_accuracy: 0.2172\n",
      "Epoch 4/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1984 - accuracy: 0.9336 - val_loss: 2.1074 - val_accuracy: 0.1379\n",
      "Epoch 5/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.1749 - accuracy: 0.9442 - val_loss: 0.2714 - val_accuracy: 0.8767\n",
      "Epoch 6/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1783 - accuracy: 0.9386 - val_loss: 0.9628 - val_accuracy: 0.5216\n",
      "Epoch 7/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.1500 - accuracy: 0.9516 - val_loss: 0.1739 - val_accuracy: 0.9865\n",
      "Epoch 8/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1446 - accuracy: 0.9530 - val_loss: 1.0526 - val_accuracy: 0.5124\n",
      "Epoch 9/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.1357 - accuracy: 0.9559 - val_loss: 0.0322 - val_accuracy: 0.9992\n",
      "Epoch 10/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1251 - accuracy: 0.9604 - val_loss: 2.0008 - val_accuracy: 0.1634\n",
      "Train on 5172 samples, validate on 1294 samples\n",
      "Epoch 1/10\n",
      "5172/5172 [==============================] - 46s 9ms/sample - loss: 0.8905 - accuracy: 0.7045 - val_loss: 2.1816 - val_accuracy: 3.8640e-04\n",
      "Epoch 2/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.3214 - accuracy: 0.8890 - val_loss: 2.3489 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.2399 - accuracy: 0.9161 - val_loss: 2.5424 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.2809 - accuracy: 0.8924 - val_loss: 2.6017 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.2238 - accuracy: 0.9186 - val_loss: 2.6639 - val_accuracy: 0.0108\n",
      "Epoch 6/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1914 - accuracy: 0.9334 - val_loss: 2.8602 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.1708 - accuracy: 0.9473 - val_loss: 1.1588 - val_accuracy: 0.4780\n",
      "Epoch 8/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.1649 - accuracy: 0.9447 - val_loss: 0.0418 - val_accuracy: 0.9930\n",
      "Epoch 9/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.1632 - accuracy: 0.9441 - val_loss: 0.0202 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.1450 - accuracy: 0.9518 - val_loss: 0.0175 - val_accuracy: 1.0000\n",
      "Train on 5172 samples, validate on 1294 samples\n",
      "Epoch 1/10\n",
      "5172/5172 [==============================] - 46s 9ms/sample - loss: 0.7729 - accuracy: 0.7431 - val_loss: 2.1204 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.2812 - accuracy: 0.9178 - val_loss: 2.3214 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.2184 - accuracy: 0.9323 - val_loss: 0.7533 - val_accuracy: 0.5108\n",
      "Epoch 4/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.1861 - accuracy: 0.9461 - val_loss: 0.0859 - val_accuracy: 0.9880\n",
      "Epoch 5/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1793 - accuracy: 0.9459 - val_loss: 2.6464 - val_accuracy: 0.0015\n",
      "Epoch 6/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.1507 - accuracy: 0.9556 - val_loss: 0.0564 - val_accuracy: 0.9946\n",
      "Epoch 7/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1849 - accuracy: 0.9434 - val_loss: 2.5665 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1442 - accuracy: 0.9566 - val_loss: 0.1815 - val_accuracy: 0.9266\n",
      "Epoch 9/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.1324 - accuracy: 0.9619 - val_loss: 0.0436 - val_accuracy: 0.9977\n",
      "Epoch 10/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1350 - accuracy: 0.9580 - val_loss: 0.0498 - val_accuracy: 1.0000\n",
      "Train on 5172 samples, validate on 1294 samples\n",
      "Epoch 1/10\n",
      "5172/5172 [==============================] - 46s 9ms/sample - loss: 1.1114 - accuracy: 0.6700 - val_loss: 1.9479 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "5172/5172 [==============================] - 29s 6ms/sample - loss: 0.8539 - accuracy: 0.7110 - val_loss: 1.5494 - val_accuracy: 0.0035\n",
      "Epoch 3/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.6731 - accuracy: 0.7106 - val_loss: 1.1344 - val_accuracy: 0.1028\n",
      "Epoch 4/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.5659 - accuracy: 0.7177 - val_loss: 0.6300 - val_accuracy: 0.4865\n",
      "Epoch 5/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.5039 - accuracy: 0.7398 - val_loss: 0.5442 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.4711 - accuracy: 0.9201 - val_loss: 0.9055 - val_accuracy: 0.3068\n",
      "Epoch 7/10\n",
      "5172/5172 [==============================] - 28s 5ms/sample - loss: 0.4556 - accuracy: 0.9067 - val_loss: 0.3820 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.4074 - accuracy: 0.9244 - val_loss: 1.2110 - val_accuracy: 0.1522\n",
      "Epoch 9/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.3788 - accuracy: 0.9292 - val_loss: 0.3632 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.3482 - accuracy: 0.9355 - val_loss: 1.4196 - val_accuracy: 0.0753\n",
      "Train on 5172 samples, validate on 1294 samples\n",
      "Epoch 1/10\n",
      "5172/5172 [==============================] - 46s 9ms/sample - loss: 1.0110 - accuracy: 0.6487 - val_loss: 1.6935 - val_accuracy: 0.2991\n",
      "Epoch 2/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.3132 - accuracy: 0.8950 - val_loss: 0.0831 - val_accuracy: 0.9969\n",
      "Epoch 3/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.2207 - accuracy: 0.9228 - val_loss: 2.4470 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1968 - accuracy: 0.9340 - val_loss: 1.7120 - val_accuracy: 0.2906\n",
      "Epoch 5/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1693 - accuracy: 0.9470 - val_loss: 2.6510 - val_accuracy: 0.0216\n",
      "Epoch 6/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1717 - accuracy: 0.9441 - val_loss: 2.1837 - val_accuracy: 0.0665\n",
      "Epoch 7/10\n",
      "5172/5172 [==============================] - 27s 5ms/sample - loss: 0.1363 - accuracy: 0.9580 - val_loss: 0.0317 - val_accuracy: 0.9977\n",
      "Epoch 8/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1380 - accuracy: 0.9581 - val_loss: 0.5030 - val_accuracy: 0.7724\n",
      "Epoch 9/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1331 - accuracy: 0.9558 - val_loss: 1.3496 - val_accuracy: 0.3690\n",
      "Epoch 10/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1301 - accuracy: 0.9580 - val_loss: 0.1639 - val_accuracy: 0.9324\n",
      "Train on 5172 samples, validate on 1294 samples\n",
      "Epoch 1/10\n",
      "5172/5172 [==============================] - 46s 9ms/sample - loss: 0.4161 - accuracy: 0.8355 - val_loss: 0.2457 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.2447 - accuracy: 0.9260 - val_loss: 1.8544 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1820 - accuracy: 0.9453 - val_loss: 2.6070 - val_accuracy: 7.7280e-04\n",
      "Epoch 4/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1529 - accuracy: 0.9546 - val_loss: 1.1040 - val_accuracy: 0.4923\n",
      "Epoch 5/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1669 - accuracy: 0.9440 - val_loss: 3.0502 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1574 - accuracy: 0.9466 - val_loss: 1.3869 - val_accuracy: 0.2998\n",
      "Epoch 7/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1481 - accuracy: 0.9506 - val_loss: 2.5380 - val_accuracy: 0.0352\n",
      "Epoch 8/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1224 - accuracy: 0.9599 - val_loss: 2.6552 - val_accuracy: 0.0881\n",
      "Epoch 9/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1091 - accuracy: 0.9640 - val_loss: 2.0346 - val_accuracy: 0.0456\n",
      "Epoch 10/10\n",
      "5172/5172 [==============================] - 26s 5ms/sample - loss: 0.1531 - accuracy: 0.9485 - val_loss: 2.5399 - val_accuracy: 0.1638\n",
      "Oops!  Nomodel to delete...\n",
      "==========================================\n",
      "result:  [[ 3.80373241 11.26967035]\n",
      " [ 3.37940302 11.88270542]\n",
      " [ 2.76345708 12.44440517]\n",
      " ...\n",
      " [ 2.5899195  12.75264937]\n",
      " [ 2.62858392 12.58371314]\n",
      " [ 2.7192172  12.41654551]]\n",
      "result[:, 1].min() :  9.596135705709457\n",
      "result[:, 1].max() :  13.090416193008423\n",
      "WRITING FILE SUCESSFULL ========!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# define undersample strategy\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import timeit\n",
    "import tracemalloc\n",
    "warnings.filterwarnings('ignore')\n",
    "# split 10 fold of undersampling\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "\n",
    "max_auc = 0.88\n",
    "\n",
    "for time in range(0, 1):\n",
    "    rsFile = \"Result_OverIteration_Bagging_Numeric/Image/TimeCalculation\"+str(time)+\".csv\"\n",
    "    f2=open(rsFile,\"w\")\n",
    "    f2.write('Time_train, Train_Peak_RAM, Time_predict, Peak_Time_prediction, thres, tn, fp, fn, tp, acc, pre, Recall, spec, F1score, mcc, auc\\n')\n",
    "    for num_Interation in ([15]):\n",
    "        \n",
    "        for epoch in ([10]):\n",
    "\n",
    "\n",
    "            print(\"working on num_Interation: \"+str(num_Interation) +\"With epoch: \"+str(epoch) )\n",
    "            X_train_List=[]\n",
    "            y_train_List = []\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            #Generating the bags\n",
    "    \n",
    "            for i in range(0, num_Interation):\n",
    "                #print(len(Normal_train_array)*1.3)\n",
    "                pneumonia = np.array(random.sample(pneumonia_trian, int(len(Normal_train_array)*1.41)))\n",
    "                normal = np.array(Normal_train_array)\n",
    "                X_train_List.append(np.concatenate((normal, pneumonia)))\n",
    "                \n",
    "                \n",
    "                y_train_List.append(np.concatenate((([0]*len(normal), [1]*(len(pneumonia))))))\n",
    "\n",
    "            X_train_List = np.array(X_train_List)\n",
    "            y_train_List = np.array(y_train_List)\n",
    "\n",
    "            X_train_List = np.array(X_train_List)\n",
    "            y_train_List = np.array(y_train_List)\n",
    "\n",
    "\n",
    "            # ============== Starting training\n",
    "            start = timeit.default_timer()\n",
    "            tracemalloc.start()\n",
    "            \n",
    "            ## Start training\n",
    "            for i in range(0, num_Interation):\n",
    "                training(X_train_List[i], to_categorical(y_train_List[i]) , i, epoch)\n",
    "                \n",
    "            current, peak = tracemalloc.get_traced_memory()\n",
    "            stop = timeit.default_timer()\n",
    "            Time_train=stop - start\n",
    "            Train_Peak_RAM = peak / 10**6\n",
    "\n",
    "            \n",
    "            \n",
    "            start = timeit.default_timer()\n",
    "            tracemalloc.start()\n",
    "            \n",
    "            predicted = model_Predict(X_test)\n",
    "            stop = timeit.default_timer()\n",
    "            Time_predict=stop - start\n",
    "            current, peak = tracemalloc.get_traced_memory()\n",
    "            Peak_Time_prediction = peak / 10**6\n",
    "            \n",
    "            \n",
    "            result = np.array([[0]*predicted[i].shape[1]]*predicted[i].shape[0])\n",
    "\n",
    "            for i in range(0, num_Interation):\n",
    "                #print(result)\n",
    "                result = result+ predicted[i]\n",
    "                ##print('*=================')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #print(y_test)\n",
    "            #print(len(result[:, 1]))\n",
    "\n",
    "\n",
    "            auc=roc_auc_score(y_test, result[:, 1]/num_Interation)\n",
    "            #print(np.argmax(y_test, axis=1))\n",
    "            print('==========================================')\n",
    "            #print(result[:, 1]/num_Interation)\n",
    "            #print(result[:, 1].shape)\n",
    "\n",
    "            if max_auc <= auc:\n",
    "                max_auc = auc\n",
    "                print('======= auc =======: ', auc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            print(\"result: \", result)\n",
    "            print(\"result[:, 1].min() : \", result[:, 1].min())\n",
    "            print(\"result[:, 1].max() : \", result[:, 1].max())\n",
    "            thres = result[:, 1].min()\n",
    "            while (thres <= result[:, 1].max()):\n",
    "            #for thres in range(1, 1001):\n",
    "                thres = thres+(result[:, 1].max()/500)\n",
    "                yhat = []\n",
    "                for i in range(len(result)):\n",
    "                    if result[i][1]>thres:\n",
    "                        yhat.append(1)\n",
    "                    else:\n",
    "                        yhat.append(0)\n",
    "\n",
    "                yhat = np.array(yhat)\n",
    "\n",
    "                tn, fp, fn, tp, acc, pre, Recall, spec, F1score, mcc = conf_matrix(y_test, yhat)\n",
    "                #auc = roc_plot(np.argmax(y_test, axis=1), yhat)\n",
    "                f2.write(str(Time_train)+\", \"+str(Train_Peak_RAM)+\", \"+str(Time_predict)+\", \"+str(Peak_Time_prediction)+\", \"+str(thres)+\", \"+str(tn)+\", \"+str(fp)+\", \"+str(fn)+\", \"+str(tp)+\", \"+str(acc)+\", \"+str(pre)+\", \"+str(Recall)+\", \"+str(spec)+\", \"+str(F1score)+\", \"+str(mcc)+\", \"+str(auc)+\"\\n\")\n",
    "\n",
    "\n",
    "    f2.close()\n",
    "    print('WRITING FILE SUCESSFULL ========!!!!!!!!!!!!!!!!!!!!!!')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-oregon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-criterion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-mambo",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-calgary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-cartridge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-granny",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-check",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-resistance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "prediction = np.argmax(result, axis=1) \n",
    "\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=prediction)\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confmat.shape[0]):\n",
    "    for j in range(confmat.shape[1]):\n",
    "        ax.text(x=j, y=i, s=confmat[i,j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')        \n",
    "plt.ylabel('true label')\n",
    "plt.show()\n",
    "\n",
    "conf_matrix(y_test, prediction)\n",
    "roc_plot(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-possible",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "for thres in range(1, 1001):\n",
    "    print('thres: ', thres/100)\n",
    "    yhat = []\n",
    "    thres=thres/100\n",
    "    for i in range(len(result)):\n",
    "        if result[i][0]>thres:\n",
    "            yhat.append(0)\n",
    "        else:\n",
    "            yhat.append(1)\n",
    "\n",
    "    yhat = np.array(yhat)\n",
    "\n",
    "    confmat = confusion_matrix(y_true=y_test, y_pred=yhat)\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "    for i in range(confmat.shape[0]):\n",
    "        for j in range(confmat.shape[1]):\n",
    "            ax.text(x=j, y=i, s=confmat[i,j], va='center', ha='center')\n",
    "    plt.xlabel('predicted label')        \n",
    "    plt.ylabel('true label')\n",
    "    plt.show()\n",
    "\n",
    "    conf_matrix(y_test, yhat)\n",
    "    roc_plot(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-principle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-sculpture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-corporation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
